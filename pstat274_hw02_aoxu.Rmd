---
title: "pstat274_hw02_aoxu"
author: "AO XU"
date: "2022-10-10"
output: html_document
---

# Problem 1

E is correct, data set I and II exhibit statistically significant autocorrelations since k $\neq$ 0 and $\rho$ > 1.

# Problem 2

## (a) 

It's stationary but not invertible.

It follows MA(2) Module, so it's stationary.

From R, we could get $B_1$ = 1, $B_2$ = -3; Since $B_1$ = 1 lies on the unit circle, it is not invertible.
```{r}
polyroot(c(1,-2/3,-1/3))
```
## (b) 

It's not stationary but invertible.

It's AR(2) module, and it could converts to (1 - $\frac{2B}{3}$ - $\frac{B^2}{3}$) $X_t$ = $Z_t$. It's invertible since we could write it as the form of "$Z_t$ = $\phi$B$X_t$".

From R, we could get $B_1$ = 1, $B_2$ = -3; Since $B_1$ = 1 lies on the unit circle, it is not stationary.
```{r}
polyroot(c(1,-2/3,-1/3))
```

# Problem 3

## (a)

MA(3), $\theta_1$ = 2, $\theta_2$ = 0.5, $\theta_3$ = -0.1

(1) The mathematical equation for MA(3) model: $X_t$ = $Z_t$ + 2$Z_{t-1}$ + 0.5$Z_{t-2}$ - 0.1$Z_{t-3}$

(2) By using the formula "$\rho_x(k)$ = $\frac{\theta_k+\theta_1\theta_{k+1}+...+\theta_{q-k}\theta_k}{1+\theta_1^2+...+\theta_q^2}$), k = 1,2,...,q, $\rho_x(k)$ = 0, k > q", we get 

$\rho(1)$ = $\frac{\theta_1+\theta_1\theta_2+\theta_2\theta_3}{1+\theta_1^2+\theta_2^2+\theta_3^2}$ = $\frac{2+2*0.5+0.5(-0.1)}{1+2^2+0.5^2+(-0.1)^2}$ = 0.56083650

$\rho(2)$ = $\frac{\theta_2+\theta_1\theta_3}{1+\theta_1^2+\theta_2^2+\theta_3^2}$ = $\frac{0.5+2(-0.1)}{1+2^2+0.5^2+(-0.1)^2}$ = 0.05703422

$\rho(3)$ = $\frac{theta_3}{1+\theta_1^2+\theta_2^2+\theta_3^2}$ = $\frac{-0.1}{1+2^2+0.5^2+(-0.1)^2}$ = -0.01901141

$\rho(4)$ = 0

```{r}
ARMAacf(ma = c(2, 0.5, -0.1), lag.max = 4, pacf = FALSE)
```

## (b) 

AR(1) $\phi_1$ = -0.5

(1) the mathematical equation for AR(1) model: $X_t$ = -0.5$X_{t-1}$ + $Z_t$

(2) 

$\rho(1)$ = $\phi_1$ = -0.5

$\rho(2)$ = ${\phi_1}^2$ = (-0.5)^2 = 0.25

$\rho(3)$ = ${\phi_1}^3$ = (-0.5)^3 = -0.125

$\rho(4)$ = ${\phi_1}^4$ = (-0.5)^4 = 0.0625

```{r}
ARMAacf(ar = -0.5, lag.max = 4, pacf = FALSE)
```

# Problem 4

Since $X_t$ = 3 + Y + Z, Y is a mean zero random variable with variance $\sigma_v^2$, independent of the white noise $Z_t$, then we could get 

E($X_t$) = E(3 + Y + $Z_t$) = 3 + E(Y) + E($Z_t$) = 3 + 0 + 0 = 3

Var($X_t$) = Var(3 + Y + $Z_t$) = 0 + Var(Y) + Var($Z_t$) + 2Cov(3,Y) + 2Cov(3,$Z_t$) + 2Cov(Y,$Z_t$) = $\sigma_Y^2+\sigma_Z^2$

$\gamma(X_t,X_{t+k})=$Cov($X_t$,$X_{t+k}$) = E($X_tX_{t+k}$) - E($X_t$)E($X_{t+k}$) = E(9 + 3Y + 3$Z_{t+k}$ + 3Y + Y^2 + Y$Z_{t+k}$ + 3$Z_t$ + Y$Z_t$ + $Z_tZ_{t+k}$) - (3 + E(Y) + E($Z_t$))(3 + E(Y) + E($Z_{t+k}$)) = 9 + $\sigma_Y^2$ - 9 = $\sigma_Y^2$

When k = 0, Cov($X_t$,$X_{t+k}$) = E($X_t^2$) - $E($X_t$)^2$ = Var($X_t$) = $\sigma_Y^2+\sigma_Z^2$

$$\gamma_x(k) = 
\begin{cases} 
  \sigma_Y^2 & k \ne 0\\ 
  \sigma_Y^2+\sigma_Z^2 & k=0
\end{cases}$$

Therefore, $X_t$ is stationary since $\mu$ is constant and $\sigma_x(k)$ doesn't depend on t.

Autocovariance function: $\gamma(X_t,X_{t+k})$ = 
$\rho_x(k) = \begin{cases} \sigma_Y^2+\sigma_Z^2 & k \ne 0\\ \sigma_Y^2 & k=0\end{cases}$

Autocorrelation function: $\rho_x(t,t+k) = Cov(X_t,X_{t+k})/\sqrt{Var(X_t)Var(X_{t+k})} =  \begin{cases} \sigma_Y^2/(\sigma_Y^2+\sigma_Z^2) & k \ne 0\\ (\sigma_Y^2+\sigma_Z^2)/(\sigma_Y^2+\sigma_Z^2) = 1 & k=0\end{cases}$

# Problem 5

$X_t$ = $Z_t$ + 2$Z_{t-1}$ - 8$Z_{t-2}$ 

## (a) 

MA(2), q = 2

## (b) 

The model is stationary but not invertible.

Since MA(2) is always stationary, the model in the problem is stationary.

$\theta(z)$ = 1 + $\theta_1$z+ $\theta_2$$z^2$

Since both roots are inside of the unit circle, the model is not invertible.
```{r}
polyroot(c(1,2,-8))
```

## (c) 

$\rho_x(2)$ = $\frac{\theta_2}{1+{\theta_1}^2+{\theta_2}^2}$ = $\frac{-8}{1+{2}^2+{(-8)}^2}$ = $-\frac{8}{69}$

```{r}
xt <- arima.sim(list(ma=c(1,2,-8)),n=300)
acf(xt, main="ACF")
acf(xt, main="ACF")$acf[3]
-8/69
# Since -0.1424422 is nearly the same as -0.115942, then my sample estimate of ÏX(2) are nearly the same as its true value found by calculations.
```

```{r}
xt <- arima.sim(list(ma=c(1,2,-8)),n=10000)
acf(xt, main="ACF")
acf(xt, main="ACF")$acf[3]
```

# G1 
$X_t$ = $Z_t$ + $\theta$$Z_{t-2}$

## (a) 

E($X_t$$X_{t+k}$) = E(($Z_t$+$\theta$)($Z_{t+k}$$Z_{t-2+k}$)) = E($Z_t$$Z_{t+k}$) + $\theta$E($Z_{t-2}$$Z_{t+k-2}$) + $\theta$E($Z_{t-2}$$Z_{t+k}$) + $\theta^2$E($Z_{t-2}$$Z_{t+k-2}$)

$\gamma_x(t,t+k)$ = E($X_t$$X_{t+k}$) - E($X_t$)E($X_{t+k}$); When k = 0, $\gamma_x(t,t+k)$ = 1 + $\theta^2$; When k = $\pm2$, $\gamma_x(t,t+k)$ = $\theta$; When k is other numbers, $\gamma_x(t,t+k)$ = 0.

Therefore, $\rho_x(k)$ = 1 when k = 0;  $\rho_x(k)$ = $\frac{\theta}{1+\theta^2}$ when k = $\pm2$; $\rho_x(k)$ = 0 when k is other numbers.

When $\theta$ = 0.8,

Autocovariance function: $\gamma_x(t,t+k)$ = 1.64 when k = 0; $\gamma_x(t,t+k)$ = 0,8 when k = $\pm2$; $\gamma_x(t,t+k)$ = 0 when k is other numbers.


Autocorrelation function: $\rho_x(k)$ = 1 when k = 0; $\rho_x(k)$ = $\frac{20}{41}$ when k = $\pm2$; $\rho_x(k)$ = 0 when k is other numbers.

## (b)

When $\theta$ = 0.8,

Var($\frac{X_1+X_2+X_3+X_4}{4}$) = $\frac{Var(X_1+X_2+X_3+X_4)}{16}$ = $\frac{1}{16}$($(Var(X_1)+Var(X_2)+Var(X_3)+Var(X_4)$ + 2(Cov($X_1X_2$)+Cov($X_1X_3$)+Cov($X_1X_4$)+Cov($X_2X_3$)+Cov($X_2X_4$)+Cov($X_3X_4$))) = $\frac{1}{16}$(4Var($X_t$)+4$\theta$) = $\frac{1}{16}$(4($Var(Z_t)$+$\theta^2Var(Z_{t-2})$+Cov($Z_t$,$Z_{t-2}$))+4$\theta$) = $\frac{1}{16}$(4(1+$\theta^2$)+4$\theta$) = $\frac{1}{16}$(4*1.64+3.2) = 0.61

## (c)

When $\theta$ = -0.8,

Var($\frac{X_1+X_2+X_3+X_4}{4}$) = $\frac{1}{16}$(4(1+$\theta^2$)+4$\theta$) = $\frac{1}{16}$(4*1.64-3.2) = 0.21

The variance in (c) is smaller than in (b) and closer to 0.

# G2

Example 1: $X_t$ = 3$X_{t-1}$ + 3$X_{t-2}$ + $Z_t$
```{r}
xt <- arima.sim(list(ma=c(1,3,3)),n=100)
acf(xt, main="ACF")
plot(xt)
```

Example 2: $X_t$ = 5$X_{t-1}$ - 8$X_{t-2}$ + $Z_t$
```{r}
xt <- arima.sim(list(ma=c(1,5,-8)),n=100)
acf(xt, main="ACF")
plot(xt)
```

# G3

$E({e}^{\displaystyle \sum_{i=1}^{n}a_ix_i})$ = E(exp($a_1$($Z_1$+$\theta$$Z_0$)+$a_2$($Z_2$+$\theta$$Z_1$)+...+$a_n$($Z_n$+$\theta$$Z_{n-1}$))) = E(exp($a_1$$\theta$$Z_0$))E(exp($a_1$+$a_2\theta$)$Z_1$))...E(exp($a_n$$Z_n$)) = m($\theta$$a_1$)m($a_1$+$theta$$a_2$)...m($a_{n-1}$+$\theta$$a_n$)m$a_n$

We find that the MGF of $x_1$,$x_2$,...,$x_n$ depends on $a_1$,$a_2$...$a_n$ and $\theta$ but not t, so $X_t$ is strictly stationary. 

E($X_t$) = E($Z_t$+$\theta$$Z_{t-1}$) = E($Z_t$)+E($\theta$$Z_{t-1}$) = 0 + $\theta(0)$ = 0;

Cov($X_{t+h}$,$X_t$) =  Cov($Z_{t+h}$+$\theta$$Z_{t+h-1}$,$Z_{t}$+$\theta$$Z_{t-1}$) = Cov($Z_{t+h},Z_t$) + $\theta$Cov($Z_{t+h},Z_{t-1}$) + $\theta$Cov($Z_{t+h-1},Z_t$) + $\theta^2$Cov($Z_{t+h-1},Z_{t-1}$) = E($Z_{t+h}Z_{t}$)+$\theta$E($Z_{t+h}Z_{t-1}$)+$\theta$E($Z_{t+h-1}Z_{t}$)+$\theta^2$E($Z_{t+h-1},Z_{t-1}$)

When h = 0, E($Z_t^2$) = Var($Z_t$) - $E(Z_t)^2$ = $\sigma_z^2$, then Cov($X_{t+h}$,$X_t$) = $\sigma_z^2$ + $\theta^2$$\sigma_z^2$;

When h = $\pm1$, Cov($X_{t+h}$,$X_t$) = $\theta$;

When h is other numbers, Cov($X_{t+h}$,$X_t$) = 0;

Then, we could get $\gamma_x(t+h,h)$ = Cov($X_{t+h}$,$X_t$) =  
$$\begin{cases} 
  \sigma_Z^2 + \theta^2\sigma_Z^2 & h = 0\\ 
  \theta & h = \pm1\\
  0 & other numbers
\end{cases}$$

Since $\mu$ is independent of t and $\gamma_x(t+h,h)$ is also independent of t, then $X_t$ is weakly stationary.

Therefore, $X_t$ is s both weakly and strictly stationary.