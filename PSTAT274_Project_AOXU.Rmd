---
title: \vspace{8.5cm} **"Time Series Analysis of Monthly Milk Production from Jan. 1962 to Dec. 1975"**
author: "Ao Xu"
date: "7 Dec 2022"
output: 
  pdf_document: null
geometry: margin=1.0in
font-size: 11.5pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE}
#Library
library(ggplot2)
library(ggfortify)
library(MASS)
library(tsdl)
library(forecast)
library(tidyverse)
library(forecast)
library(qpcR)
library(GeneCycle)
```

\newpage

```{r, include=FALSE}
#tsdl
length(tsdl[[203]])
attr(tsdl[[203]],"subject")
attr(tsdl[[203]],"source")
attr(tsdl[[203]],"description")
```

# **ABSTRACT**
Based on monthly measurements gathered between the years 1962 and 1975, the time series analysis of milk production (pounds per cow) is performed in this project. The information was gathered in the R tsdl data library. The reference number is 203. There are 168 observations in the original dataset. The original dataset was split into two sets. The first 156 observations are in the training set, and the final 12 measurements are in the testing set. Applying the Box-Jenkins methodology, a suitable model is created from the training set to project future values and compare them to values from the testing set. The final model we selected is $SARIMA(0,1,1)(0,1,4)_{12}$. All predicted values fall within the 95% confidence interval according to this model.

# **1.0 INTRODUCTION**

The dataset, which spans the period of January 1962 to December 1975, has 168 measures of monthly milk production (pounds per cow). It is a typical time series dataset with a enough number of observations for analysis and the creation of a prediction model. The original dataset is divided into a training set and a test set so that the prediction model's accuracy can be assessed. Monthly measurements from January 1962 to December 1974 make up the 156 values in the training set (denoted as c_train). Twelve values, representing the monthly measurements from January 1975 to December 1975, make up the testing set (denoted as c_test). We initially choose many models to estimate their coefficients using the Box-Jenkins approach. Then, by comparing AICcs and using diagnostic testing, we evaluate the models to determine which is best for forecasting. In order to verify the accuracy of our final model, we compare the forecast values to the data from the testing set.

##  *1.1 Data Processing*
The original data's plot displays recurring patterns and an upward trend, which point to seasonality and a non-constant mean. It appears that the data's variance is stable, nevertheless.
```{r, include=FALSE}
milk <- tsdl[[203]]
```

```{r , echo=FALSE, fig.cap="Monthly Measurements of Milk Prodcution Jan 1962 â€“ Dec 1975."}
plot.ts(milk,xlab = "",main = "")
```

\newpage
```{r, include=FALSE}
# Divide it into train and test set
c_train <- ts(milk[1:156],start = c(1962,1),frequency = 12)
c_test <-ts(milk[157:168],start = c(1975,1),frequency = 12)
```

We plot the histogram and ACF of the original dataset to further investigate its non-stationarity. The histogram has a right-skewed bias and is not symmetrical. Because the x axis is based on year, the ACF reveals a seasonality of 12 and a slow decay (shown in the graph as lag 1).

```{r , echo=FALSE, fig.cap="Histogram and ACF plot of c_train"}
# Plot the hist and acf
par(mfrow=c(1,2))
hist(c_train, col = "light blue",main = "", xlab ="")
acf(c_train, lag.max = 40, main = "", xlab ="")
```

Since the histogram does not exhibit a symmetric, normal pattern, we apply a box-cox transformation and refer to the resultant data as milk.log after log transformation.

```{r , fig.width=7,fig.height=4, echo=FALSE, fig.cap="Box-cox transformation on c_train"}
# Perform box-cox transformation on c_train
bcTransform <- boxcox(c_train~as.numeric(1:length(c_train))) 
lambda = bcTransform$x[which(bcTransform$y == max(bcTransform$y))] 
milk.bc = (1/lambda)*(c_train^lambda-1)
lambda
```

The box-cox plot gives us a result of $\lambda = 0.2626263$. The confidence interval of box-cox transformation includes 0 and 1. Therefore, we could experiment with non-transformation, log transformation, and box-cox transformation. We discovered that the log transformation was the most appropriate method for this data after completing the entire process and comparing the outcomes for each transformation. We set our log-transformed data set be milk.log.

```{r, include=FALSE}
# Perform log transformation on c_train
milk.log <- log(c_train) 
plot.ts(milk.log)
```

```{r, fig.width=7,fig.height=3.8,echo=FALSE,fig.cap="Comparison on c_train and milk.log: *Top Left*: Time Series Plot on c_train; *Top Middle*: Histogram of c_train; *Top Right*: Normal Q-Q Plot of c_train; *Bottom Left*: Histogram of milk.log; *Bottom Middle*: Histogram of milk.log; *Bottom Right*: Normal Q-Q Plot of milk.log"}
# plot and compare the difference
par(mfrow=c(2,3))
plot.ts(c_train,xlab = "", main = "")
hist(c_train, col = "light blue", xlab = "", main = "")
qqnorm(c_train, main = "", xlab = "")
qqline(c_train, col = "red")

plot.ts(milk.log,xlab = "", main = "")
hist(milk.log, col = "light blue", xlab = "", main = "")
qqnorm(milk.log, main = "", xlab = "")
qqline(milk.log, col = "red")
```

We can further ensure that the log transformation is suitable for our data by contrasting the time series plot, histogram, and Q-Q plots of c_train and milk.log. We shall execute log transformation and continue working with milk.log. The next thing we'll do is to decompose the seasonality and trend of milk.log. 

```{r, fig.width=5.5,fig.height=3, echo=FALSE, fig.cap="c_train: Decomposition of additive time series"}
# plot the decomposition
x1 <- ts(as.ts(milk.log),frequency = 12)
decomp <- decompose(x1)
plot(decomp, xlab = "")
```

According on the decomposition plot, we predict that the difference between $\nabla_{12}$ and $\nabla_{1}$ will cause our dataset to become stationary.

The graph doesn't reveal any repeating patterns after we differentiate milk.log at $\nabla_{12}$. However, a negative trend is still clearly visible. We therefore differenciate once more at $\nabla_{1}$. Compared to the original, $\nabla_{1}$$\nabla_{12}$milk.log appears to be stationary. The graph's mean is also close to zero.

\newpage
```{r, include=FALSE}
# ln(Ut) differenced at lag 12
var(milk.log)
milk.log_12 <- diff(milk.log, lag=12)
var(milk.log_12)
# ln(Ut) differenced at lag 12 and then lag 1
milk.log_12_1 <- diff(milk.log_12, lag=1)
var(milk.log_12_1)
# ln(Ut) differenced at lag 12, then lag 1 and then lag 1
milk.log_12_1_1 <- diff(milk.log_12_1, lag=1)
var(milk.log_12_1_1)
```

```{r, fig.width=7.5,fig.height=3.3, echo=FALSE, fig.cap="*Left*: Time Series Plot of $\\nabla12$$ln(U_t)$; *Right*: Time Series Plot of $\\nabla1$$\\nabla12$$ln(U_t)$"}
par(mfrow=c(1,2))
plot.ts(milk.log_12, main = "", xlab = "")
abline(h=mean(milk.log_12), col = "blue")
plot.ts(milk.log_12_1, main = "", xlab = "")
abline(h=mean(milk.log_12_1), col = "blue")
```

We keep making differences at lag 1. Our decision on the number of differences is finalized by comparing $\nabla_{1}$$\nabla_{12}$milk.log, and $\nabla_{1}$$\nabla_{1}$$\nabla_{12}$milk.log.

```{r, echo=FALSE}
v <- cbind(var(milk.log), var(milk.log_12), var(milk.log_12_1), var(milk.log_12_1_1))
rownames(v) <- "Variance"
colnames(v) <- c("$ln(U_t)$","$\\nabla_{12}$$ln(U_t)$",
                 "$\\nabla_{1}$$\\nabla_{12}$$ln(U_t)$","$\\nabla_{1}$$\\nabla_{1}$$\\nabla_{12}$$ln(U_t)$")
knitr::kable(v)
```

By comparing the variances above, we get the smallest variance is 0.0001696	when differenciating it at lag 12 and lag 1. However, it goes up to 0.0004243 with an additional difference at lag 1. As a result, we should stop the difference and choose $\nabla_{1}$$\nabla_{12}$milk.log.

```{r, fig.width=7,fig.height=4, echo=FALSE, fig.cap="*Top Left*: ACF of $ln(U_t)$; *Top Middle*: ACF of $\\nabla12$$ln(U_t)$; *Top Right*: ACF of $\\nabla1$$\\nabla12$$ln(U_t)$; *Bottom Left*: PACF of $ln(U_t)$; *Bottom Middle*: PACF of $\\nabla12$$ln(U_t)$; *Bottom Right*: PACF of $\\nabla1$$\\nabla12$$ln(U_t)$"}
#plot and compare the ACFs and PACFs
par(mfrow=c(2,3))
acf(milk.log, lag.max = 60, main = expression(ln(U[t])))
acf(milk.log_12, lag.max = 60, main = expression(nabla[12]~~ln(U[t])))
acf(milk.log_12_1, lag.max = 60, main = expression(nabla[1]~nabla[12]~~ln(U[t])))
pacf(milk.log, lag.max = 60, main = "")
pacf(milk.log_12, lag.max = 60, main = "")
pacf(milk.log_12_1, lag.max = 60, main = "")
```

The graphs below show a comparison of the ACF and PACF for milk.log, $\nabla_{12}$milk.log, and $\nabla_{1}$$\nabla_{12}$milk.log. After being differentiated at lag 12, $\nabla_{12}$milk.log does not exhibit spikes at multiples of lag 12. It still exhibits a declining pattern, indicating non-stationary. For $\nabla_{1}$$\nabla_{12}$milk.log, there are no decaying patterns after differenciating it at lag 1. A stationary process is indicated by the ACF and PACF plots of $\nabla_{1}$$\nabla_{12}$milk.log.

\newpage
## *1.2 Model Identification*
```{r, fig.width=7,fig.height=4,echo = FALSE,fig.cap="ACF and PACF of $\\nabla1$$\\nabla12$$ln(U_t)$"}
par(mfrow=c(2,3))
acf(milk.log_12_1, lag.max = 60, main = expression(nabla[1]~nabla[12]~~ln(U[t])))
pacf(milk.log_12_1, lag.max = 60, main = expression(nabla[1]~nabla[12]~~ln(U[t])))
```

We choose to apply a SARIMA model for this dataset. Since we difference at lag 1 and lag 12, d = 1 and D = 1. The ACF plot of $\nabla_{1}$$\nabla_{12}$milk.log shows spikes at lag 1, 12, 13 and 48. Then it cuts off at lag 48. Lag 13 have values outside of the confidence interval probably because of the influence from lag 1. Therefore, q = 1 and Q = 1 or 4. We see an alternating decaying pattern in PACF, which suggest P = 1 and the model might be a pure MA model. Within lag 12, PACF is significant at lag 1. Therefore, p = 1 and P = 1.


## *1.3 Model Estimation*
The possible model suggested by ACF and PACF are $SARIMA(0,1,1)(0,1,1)_{12}$ and $SARIMA(0,1,1)(0,1,4)_{12}$. We compare the AICc of them.

```{r, include=FALSE}
# Compare AICcs
a <- AICc(arima(milk.log, order = c(0,1,1), seasonal = list(order = c(0,1,1), period = 12), method = "ML"))
b <- AICc(arima(milk.log, order = c(0,1,1), seasonal = list(order = c(0,1,4), period = 12), method = "ML"))
```

```{r, echo = FALSE,fig.cap="Comparison of AICc among possible models"}
AICc_value <- rbind(a,b)
rownames(AICc_value) <- c("$SARIMA(0,1,1)(0,1,1)_{12}$", "$SARIMA(0,1,1)(0,1,4)_{12}$")
colnames(AICc_value) <- c("AICc")
knitr::kable(AICc_value)
```

$SARIMA(0,1,1)(0,1,4)_{12}$ have the lowest AICc values. Therefore, we are going to pick $SARIMA(0,1,1)(0,1,4)_{12}$ for further analysis. 

## *1.4 Model Checking*
### Model: $SARIMA(0,1,1)(0,1,1)_{12}$
```{r}
arima(milk.log, order = c(0,1,1), seasonal = list(order = c(0,1,4), period = 12), method = "ML")
```

The confidence intervals of sma2 and sma3 contain 0, so they are necessary to be fixed by zero.

```{r}
arima(milk.log, order=c(0,1,1), seasonal = list(order = c(0,1,4), period = 12), fixed = c(NA,NA,0,0,NA), method="ML")
```

```{r}
polyroot(c(1, -0.1729))
```
The root is outside unit circle, indicating the model is invertible. 

```{r, include = FALSE}
fit <- arima(milk.log, order = c(0,1,1), seasonal = list(order = c(0,1,4), period = 12), method = "ML")
res <- residuals(fit)
```

The next step is to look at the Model residual. No trend, no seasonality, and no discernible variance change can be seen in the residual plot. Its mean is close to zero.

```{r , echo=FALSE, fig.width=7.2,fig.height=2.2, fig.cap="Time Series Plot of Model 1 $SARIMA(0,1,1)(0,1,4)_{12}$ residual"}
plot.ts(res, xlab = "")
abline(h=mean(res),col = "blue")
```

The histogram looks normal and the mean of it is nearly at zero. Also, the majority of the data lies on the qqline.

```{r , echo=FALSE, fig.width=7.2,fig.height=3.2, fig.cap="Model 1 $SARIMA(0,1,1)(0,1,4)_{12}$: *Left*: Histogram of residual; *Right*: Normal Q-Q Plot of residual"}
# plot the histogram and qq plot
par(mfrow=c(1,2))
hist(res, density = 20, breaks = 20, col = "blue", xlab = "", prob = TRUE, main = "")
mean <- mean(res)
std <- sqrt(var(res))
curve(dnorm(x,mean,std),add = TRUE)
qqnorm(res,main = "", xlab = "")
qqline(res, col = "blue")
```

The ACF and PACF graphs show that Lag 4 is somewhat outside the 95% confidence interval. However, it can be counted as 0. All of other lags fall inside the confidence interval. We can infer that the residual of the model follows the white noise.


```{r , echo=FALSE, fig.width=7.2,fig.height=3.2,fig.cap="Model 1 $SARIMA(0,1,1)(0,1,4)_{12}$: *Left*: ACF of residual; *Right*: PACF of residual"}
# acf and pacf
par(mfrow=c(1,2))
acf(res,lag.max=60, main = "", xlab = "")
pacf(res,lag.max=60, main = "", xlab = "")
```

\newpage
## *1.5 Model Diagnostics*
### Model: $SARIMA(0,1,1)(0,1,4)_{12}$ 

Since $\sqrt{156}$ $\approx$ 12.49, we set lag = 12 to perform model diagnostics. Also, since there are three parameters at the model functions, we set fitdf = 3 to do Box-Pierce test and Box-Ljung test .

```{r}
# Diagnostic test
shapiro.test(res)
Box.test(res,lag = 12, type = c("Box-Pierce"),fitdf = 3) # 3 parameters
Box.test(res,lag = 12, type = c("Ljung-Box"),fitdf = 3) # 3 parameters
Box.test(res^2,lag = 12, type = c("Ljung-Box"),fitdf = 0)
ar(res,aic=TRUE,order.max=NULL, mehod = c("yule-walker"))
```
It passes all the diagnostic tests but not Shapiro-Wilk normality test. 
That's because the originial data is non-gaussian distribution, but the box-cox jenkins is already the best one.
It cannot pass the Shapiro_Wilk normality test due to the influence of heavy tail. Also, there are two leverage points on the right side at qq-norm, which can affect the residual's normality.

## *1.6 Spectral Analysis*
```{r, include=FALSE}
# install.packages("TSA")
require(TSA)
```

We investigate the periodicity of the Model residual before proceeding to our final forecasting step. The absence of strong spikes in the periodogram suggests that the residual lacks periodicity. We also fail to reject the null hypothesis that the residual is white noise in the Kolmogorov-Smirnov test.

```{r , echo=FALSE, fig.width=7.5,fig.height=3.3, fig.cap="Model $SARIMA(0,1,1)(0,1,4)_{12}$: *Left*: Periodogram of residual; *Right*: Kolmogorov-Smirnov Test"}
par(mfrow=c(1,2))
periodogram(res, main = "")
abline(h = 0)
cpgram(res, main = "") 
```

```{r}
fisher.g.test(res)
```


With a value of 0.7995265, the fisher test cannot reject the null hypothesis that the residual is Gaussian white noise. Then, we are confident to use our model for predicting after carefully examining the residual plots and passing the normality and periodicity tests.

Therefore, our final model is $SARIMA(0,1,1)(0,1,4)_{12}$ $$\nabla_{1}\nabla_{12}ln(U_t) = (1-0.1729_{(0.0866)}B)(1-0.6771_{(0.0901)}B^{12}+0.3054_{(0.1200)}B^{48})Z_t$$ with $Zt WN(0,\sigma_z^2)$  and $\sigma_z^2 = 9.928*10^{-5}$

\newpage
## *1.7 Data Forecast*
```{r, include = FALSE}
# get git.1
fit.1<-arima(milk.log, order = c(0,1,1), seasonal = list(order = c(0,1,4), period = 12),fixed = c(NA,NA,0,0,NA), method = "ML")
```

Firstly, we forecast on the transformed data by time series.

```{r, echo=FALSE, fig.width=7,fig.height=4, fig.cap="Forecast on the transformed data by time series"}
# forecast on the transformed data
pred.tr <- predict(fit.1, n.ahead = 12)
U.tr = pred.tr$pred + 2*pred.tr$se
L.tr = pred.tr$pred - 2*pred.tr$se
ts.plot(milk.log,xlim = c(1962,1976), xlab = "", ylim= c(6.3,7.0))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
legend("bottomright", c("Prediction","95% C.I."), fill = c("red", "blue"), cex = 1.25)
```

Next, we forecast on the original data by time series.

```{r, echo=FALSE, fig.width=7,fig.height=4, fig.cap="Forecast on the original data by time series"}
# forecast on the original data
pred.orig <- exp(pred.tr$pred)
U = exp(U.tr)
L = exp(L.tr)
ts.plot(c_train, xlim= c(1962,1976), ylim= c(550,1050))
lines(U, col="blue", lty="dashed")
lines(L, col="blue", lty="dashed")
points(pred.orig,col = "red", pch = 1)
legend("bottomright", c("Prediction","95% C.I."), fill = c("red", "blue"), cex = 1.25)
```

Then, we zoom in the forecast on the original data by time series.

```{r, echo=FALSE, fig.width=7,fig.height=4, fig.cap="Zoomed in: Forecast on the original data set by time series"}
# zoom in
pred.orig <- exp(pred.tr$pred)
ts.plot(c_train, xlim= c(1973,1976),ylim=c(650,1050))
lines(U, col="blue", lty="dashed")
lines(L, col="blue", lty="dashed")
points(pred.orig,col = "red", pch = 1)
legend("bottomright", c("Prediction","95% C.I."), fill = c("red", "blue"), cex = 1.25)
```

\newpage
Finally, we forecast on the original data set by time series with c_test data.

```{r , echo=FALSE, fig.width=7,fig.height=4, fig.cap="Zoomed in: Forecast on the original data set by time series with c_test data"}
# zoom in
pred.orig <- exp(pred.tr$pred)
ts.plot(milk, xlim= c(1973,1976),ylim=c(600,1050))
lines(U, col="blue", lty="dashed")
lines(L, col="blue", lty="dashed")
points(pred.orig,col = "red", pch = 1)
points(c_test,col = "black", pch = 20)
legend("bottomright", c("Prediction","95% C.I.","c_test"), fill = c("red", "blue","black"), cex = 1.25)
```

All testing set data fall inside the 95% confidence interval of prediction model. The numbers predicted by the chosen model are also fairly similar to the actual values, demonstrating the suitability and sufficiency of our model for forecasting milk production.

# **2.0 CONCLUSION**
After comparing AICc values of two possible models - $SARIMA(0,1,1)(0,1,1)_{12}$ and $SARIMA(0,1,1)(0,1,4)_{12}$. The model $SARIMA(0,1,1)(0,1,4)_{12}$ has smaller AICc and provide credible forecasting results for our data set. It passes all diagnostic tests but not Shapiro-Wilk normality test. That's because the originial data is non-gaussian distribution, but the box-cox jenkins is already the best one. Also, it cannot pass the Shapiro_Wilk normality test due to the influence of heavy tail. There are two levergae opints on the right side at the qq norm plot, which can affect the residual's normality. However, we are still confident to claim that our $SARIMA(0,1,1)(0,1,4)_{12}$ model is appropriate to forecast the monthly measurements of milk production (pounds per cow).

The final model is: 
$$\nabla_{1}\nabla_{12}ln(U_t) = (1-0.1729_{(0.0866)}B)(1-0.6771_{(0.0901)}B^{12}+0.3054_{(0.1200)}B^{48})Z_t$$ with $$Z_t \sim WN(0,\sigma_z^2)$$  and $$\sigma_z^2 = 9.928*10^{-5}$$
\newpage
# **3.0 REFERENCES**

Feldman, R. (2022, December). *PSTAT 274 - Lecture 15. Fall 2022*. Santa Barbara; University of California, Santa Barbara. 

Feldman, R. (2022, December). *PSTAT 274 - Lab 7. Fall 2022*. Santa Barbara; University of California, Santa Barbara. 

Hyndman, R., & Yang, Y. (2018). tsdl: Time Series Data Library. v0.1.0.

# **4.0 APPENDIX**

## 4.1 Library Used
```{r, eval=FALSE}
library(tsdl)
library(forecast)
library(tidyverse)
library(MASS)
library(ggplot2)
library(ggfortify)
library(forecast)
library(GeneCycle)
library(qpcR)
require(TSA)
```

## 4.2 Data Processing
```{r, eval=FALSE}
# View the data set NO.203 in tsdl
length(tsdl[[203]])
attr(tsdl[[203]],"subject")
attr(tsdl[[203]],"source")
attr(tsdl[[203]],"description")
# View the Original Data
milk <- tsdl[[203]]
plot.ts(milk,xlab = "",main = "")
# Set up training and testing group
# c_train totally 156 points, c_test totally 12 points
c_train <- ts(milk[1:156],start = c(1962,1),frequency = 12)
c_test <-ts(milk[157:168],start = c(1975,1),frequency = 12)
# Show histogram and acf plot of c_train
par(mfrow=c(1,2))
hist(c_train, col = "light blue",main = "")
acf(c_train, lag.max = 40, main = "")
# Perform box-cox transformation on c_train
bcTransform <- boxcox(c_train~as.numeric(1:length(c_train)))
lambda = bcTransform$x[which(bcTransform$y == max(bcTransform$y))]
c_train.bc = (1/lambda)*(c_train^lambda-1)
# Perform log transformation on c_train
milk.log <- log(c_train) 
plot.ts(milk.log)
# Compare c_train and milk.log
par(mfrow=c(2,3))
plot.ts(c_train,xlab = "", main = "")
hist(c_train, col = "light blue", xlab = "", main = "")
qqnorm(c_train, main = "", xlab = "")
qqline(c_train, col = "red")
plot.ts(milk.log,xlab = "", main = "")
hist(milk.log, col = "light blue", xlab = "", main = "")
qqnorm(milk.log, main = "", xlab = "")
qqline(milk.log, col = "red")
# Decompose the trend and seasonality of c_train
x1 <- ts(as.ts(milk.log),frequency = 12)
decomp <- decompose(x1)
plot(decomp, xlab = "")
# Remove seasonality and trend
# ln(Ut) differenced at lag 12
var(milk.log)
milk.log_12 <- diff(milk.log, lag=12)
var(milk.log_12)
# ln(Ut) differenced at lag 12 and then lag 1
milk.log_12_1 <- diff(milk.log_12, lag=1)
var(milk.log_12_1)
# ln(Ut) differenced at lag 12, then lag 1 and then lag 1
milk.log_12_1_1 <- diff(milk.log_12_1, lag=1)
var(milk.log_12_1_1)
# Check differencing
par(mfrow=c(1,2))
plot.ts(milk.log_12, main = "")
abline(h=mean(milk.log_12), col = "blue")
plot.ts(milk.log_12_1, main = "")
abline(h=mean(milk.log_12_1), col = "blue")
# Create table to compare variance
v <- cbind(var(c_train), var(milk.log_12), var(milk.log_12_1), var(milk.log_12_1_1))
rownames(v) <- "Variance"
colnames(v) <- c("$U_t$","$\\nabla_{12}$$U_t$",
                 "$\\nabla_{1}$$\\nabla_{12}$$U_t$",
                 "$\\nabla_{1}$$\\nabla_{1}$$\\nabla_{12}$$U_t$")
knitr::kable(v)
# Compare the ACF and PACF of milk.log, milk.log_12, and milk.log_12_1
par(mfrow=c(2,3))
acf(milk.log, lag.max = 60, main = expression(ln(U[t])))
acf(milk.log_12, lag.max = 60, main = expression(nabla[12]~~ln(U[t])))
acf(milk.log_12_1, lag.max = 60, main = expression(nabla[1]~nabla[12]~~ln(U[t])))
pacf(milk.log, lag.max = 60, main = "")
pacf(milk.log_12, lag.max = 60, main = "")
pacf(milk.log_12_1, lag.max = 60, main = "")
# Decide the proper p,d,q,P,D,Q from the acf and pacf plots
acf(milk.log_12_1, lag.max = 60, main = expression(nabla[1]~nabla[12]~~ln(U[t])))
pacf(milk.log_12_1, lag.max = 60, main = "")
```

## 4.3 Model Estimation
```{r, eval=FALSE}
# Calculate and compare the AICc for SARIMA (0,1,1)x(0,1,1) and SARIMA (0,1,1)x(0,1,4)
a <- AICc(arima(milk.log, order = c(0,1,1), seasonal = list(order = c(0,1,1), 
                                                            period = 12), method = "ML"))
b <- AICc(arima(milk.log, order = c(0,1,1), seasonal = list(order = c(0,1,4), 
                                                            period = 12), method = "ML"))
AICc_value <- rbind(a,b)
rownames(AICc_value) <- c("$SARIMA(0,1,1)(0,1,1)_{12}$", "$SARIMA(0,1,1)(0,1,4)_{12}$")
colnames(AICc_value) <- c("AICc")
knitr::kable(AICc_value)
```

## 4.4 Model Identification
## $SARIMA(0,1,1)(0,1,4)_{12}$
```{r, eval=FALSE}
# Estimate coefficients of the model
arima(milk.log, order = c(0,1,1), seasonal = list(order = c(0,1,4),
      period = 12), method = "ML")
arima(milk.log, order=c(0,1,1), seasonal = list(order = c(0,1,4),
      period = 12), fixed = c(NA,NA,0,0,NA), method="ML")
# Calculate the roots for check invertibility
polyroot(c(1, -0.1729))
# Plot the residuals
fit <- arima(milk.log, order = c(0,1,1), seasonal = list(order = c(0,1,4),
             period = 12), method = "ML")
res <- residuals(fit)
plot.ts(res, xlab = "")
abline(h=mean(res),col = "blue")
# Plot the histogram, Q-Q normal plot, ACF and PACF of the residual
par(mfrow=c(1,2))
hist(res, density = 20, breaks = 20, col = "blue", xlab = "",
     prob = TRUE, main = "")
mean <- mean(res)
std <- sqrt(var(res))
curve(dnorm(x,mean,std),add = TRUE)
qqnorm(res,main = "", xlab = "")
qqline(res, col = "blue")
par(mfrow=c(1,2))
acf(res,lag.max=60, main = "", xlab = "")
pacf(res,lag.max=60, main = "", xlab = "")
```

## 4.6 Model Diagnostics
```{r, eval=FALSE}
# Perform test on Model's residual
shapiro.test(res1)
Box.test(res1,lag = 20, type = c("Box-Pierce"),fitdf = 2)
Box.test(res1,lag = 20, type = c("Ljung-Box"),fitdf = 2)
Box.test(res1^2,lag = 20, type = c("Ljung-Box"),fitdf = 0)
ar(res1,aic=TRUE,order.max=NULL, mehod = c("yule-walker"))
```

## 4.7 Spectral Analysis
```{r, eval=FALSE}
# install.packages("TSA")
require(TSA)
par(mfrow=c(1,2))
# Graph the periodogram of Model residual
periodogram(res, main = "")
abline(h = 0)
# Perform Kolmogorov-Smirnov Test on Model residual
cpgram(res, main = "")
# Perform fisher test on Model residual
fisher.g.test(res)
```

## 4.8 Data Forecast
```{r, eval=FALSE}
fit.1<-arima(milk.log, order = c(0,1,1), 
             seasonal = list(order = c(0,1,4), 
             period = 12),
             fixed = c(NA,NA,0,0,NA), 
             method = "ML")
# Forecast on transformed data:
pred.tr <- predict(fit.1, n.ahead = 12)
U.tr = pred.tr$pred + 2*pred.tr$se
L.tr = pred.tr$pred - 2*pred.tr$se
ts.plot(milk.log,xlim = c(1962,1976), xlab = "", ylim= c(6.3,7.0))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
legend("bottomright", c("Prediction","95% C.I."), 
       fill = c("red", "blue"), cex = 1.25)
# Forecast on original data
pred.orig <- exp(pred.tr$pred)
U = exp(U.tr)
L = exp(L.tr)
ts.plot(c_train, xlim= c(1962,1976), ylim= c(550,1050))
lines(U, col="blue", lty="dashed")
lines(L, col="blue", lty="dashed")
points(pred.orig,col = "red", pch = 1)
legend("bottomright", c("Prediction","95% C.I."), 
       fill = c("red", "blue"), cex = 1.25)
# Zoom in graph
pred.orig <- exp(pred.tr$pred)
ts.plot(c_train, xlim= c(1973,1976),ylim=c(650,1050))
lines(U, col="blue", lty="dashed")
lines(L, col="blue", lty="dashed")
points(pred.orig,col = "red", pch = 1)
legend("bottomright", c("Prediction","95% C.I."), 
       fill = c("red", "blue"), cex = 1.25)
# Forecast with test dataset
pred.orig <- exp(pred.tr$pred)
ts.plot(milk, xlim= c(1973,1976),ylim=c(600,1050))
lines(U, col="blue", lty="dashed")
lines(L, col="blue", lty="dashed")
points(pred.orig,col = "red", pch = 1)
points(c_test,col = "black", pch = 20)
legend("bottomright", c("Prediction","95% C.I.","c_test"),
       fill = c("red", "blue","black"), cex = 1.25)
```







